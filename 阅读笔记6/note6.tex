\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{epsfig}
\usepackage{color}
\usepackage{mdframed}
\usepackage{lipsum}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{diagbox}
\usepackage{float}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmicx}  
\usepackage{algpseudocode}
\usepackage{amsmath} 
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{url}
\newmdtheoremenv{thm-box}{myThm}
\newmdtheoremenv{prop-box}{Proposition}
\newmdtheoremenv{def-box}{定义}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
	numbers=left, 
	numberstyle= \tiny, 
	keywordstyle= \color{ blue!70},
	commentstyle= \color{red!50!green!50!blue!50}, 
	frame=shadowbox, % 阴影效果
	rulesepcolor= \color{ red!20!green!20!blue!20} ,
	escapeinside=``, % 英文分号中可写入中文
	xleftmargin=2em,xrightmargin=2em, aboveskip=1em,
	framexleftmargin=2em
} 

\usepackage{booktabs}

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.5in}
\setlength{\topmargin}{-0.5in}

% \setlength{\textheight}{9.5in}
%%%%%%%%%%%%%%%%%%此处用于设置页眉页脚%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}                                
\usepackage{lastpage}                                           
\usepackage{layout}                                             
\footskip = 10pt 
\pagestyle{fancy}                    % 设置页眉                 
\lhead{研一下学期}                    
\chead{论文阅读笔记}                                                
% \rhead{第\thepage/\pageref{LastPage}页} 
\rhead{Step6}                                                                                               
\cfoot{\thepage}                                                
\renewcommand{\headrulewidth}{1pt}  			%页眉线宽，设为0可以去页眉线
\setlength{\skip\footins}{0.5cm}    			%脚注与正文的距离           
\renewcommand{\footrulewidth}{0pt}  			%页脚线宽，设为0可以去页脚线

\makeatletter 									%设置双线页眉                                        
\def\headrule{{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
\hrule\@height 1.0pt \@width\headwidth\vskip1pt	%上面线为1pt粗  
\hrule\@height 0.5pt\@width\headwidth  			%下面0.5pt粗            
\vskip-2\headrulewidth\vskip-1pt}      			%两条线的距离1pt        
 \vspace{6mm}}     								%双线与下面正文之间的垂直间距              
\makeatother  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{definition}{Definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\usepackage{multirow}

%--

%--
\begin{document}
\title{论文阅读笔记\\
Step6}
\author{MF1833063, 史鹏, spwannasing@gmail.com}
\maketitle


\newpage
\section{Cognitive Graph for Multi-Hop Reading Comprehension at Scale}
这篇文章基于Bert和GNN，在迭代中逐步构建出cognitive graph：$\mathcal{G}$，图中的每一个节点都和一个实体或者一个可能的答案有关。
由两部分组成：implicit extraction（System 1）和explicit reasoning（System 2）。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{1-1.png}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{1-3.png}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{1-2.png}
\end{figure}
[注] 本文选取的数据集是HotpotQA，HotpotQA 是一个大型问答数据集，它包含约 11.3 万个具备上述特征的问答对。也就是说，这些问题要求问答系统能够筛选大量的文本文档，以找到与生成答案相关的信息，并对找到的多个支撑性事实进行多步推理，从而得出最终答案。

System 1从段落中提取与问题相关的实体和answer candidate，并对其语义信息进行编码。提取的实体被组织成一个Cognitive Graph。
然后，系统2对图进行推理，并收集线索指导系统1更好地提取下一跳实体。

System 1(Bert)：\\
\begin{equation}
	\underbrace{[C L S] Q u e s t i o n[S E P] \operatorname{clues}[x, \mathcal{G}][S E P]}_{\text { Sentence } A} \underbrace{\operatorname{Para}[x]}_{\text { Sentence } B}
	\end{equation}

System 2(GNN)： \\
\begin{equation}
\begin{array}{l}{\Delta=\sigma\left(\left(A D^{-1}\right)^{T} \sigma\left(\mathbf{X} W_{1}\right)\right)} \\ {\mathbf{X}^{\prime}=\sigma\left(\mathbf{X} W_{2}+\Delta\right)}\end{array}
\end{equation}
\newpage
\section{Exploiting Explicit Paths for Multi-hop Reading Comprehension}
本文基于WikiHop数据集，在该数据集中问题以三元组的形式出现$\left(h_{e}, r, ?\right)$，$h_e$代表head entity，r代表head entity和未知的tail entity之间的关系。
任务是从给定的candidates集合中选出一个答案:$\left(c_{1}, c_{2}, \dots, c_{N}\right)$。

所做的工作是在预测答案的同时，将推理的path展示出来。方法很intuitive，但难的是想到并去做这个工作。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{2-1.png}
	\caption{Architecture of the proposed model}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{2-2.png}
	\caption{Architecture of the proposed path scoring module}
\end{figure}
\newpage
模型的流程是：
\begin{enumerate}
	\item 提取Path
	在含有$h_e$的第一篇文章中，找出在那句话或者下一句话中出现的所有实体，然后在其它的passage中寻找这些实体，如果该passge也包含candidate中的单词那么则构建出了一个path。
	为每一个candidate构建一个从$h_e$的路径。
	\item 对所有的path进行encoding以及score，选出answer以及输出path。
	\subitem[1] Context-based Path Encoding:

	对于2-hop的path：$(h_e,e_1),(e_1,c_k)$:
	\begin{equation}
		\mathbf{g}_{h_{e}}=\mathbf{s}_{p_{1}, i_{1}} \| \mathbf{s}_{p_{1}, i_{2}}
		\end{equation}
		\begin{equation}
			\mathrm{FFL}(\mathbf{a}, \mathbf{b})=\tanh \left(\mathbf{a} \mathbf{W}_{a}+\mathbf{b} \mathbf{W}_{b}\right)
			\end{equation}
		\begin{equation}
			\mathbf{r}_{h_{e}, e_{1}}=\operatorname{FFL}\left(\mathbf{g}_{h_{e}}, \mathbf{g}_{e_{1}}\right)
			\end{equation}
			\begin{equation}
				\mathbf{x}_{c t x}=\operatorname{FFL}\left(\mathbf{r}_{h_{e}, e_{1}}, \mathbf{r}_{e_{1}, c_{k}}\right)
				\end{equation}
	\subitem[2] Passage-based Path Encoding:\\
	首先计算相似矩阵：$\mathbf{A}_{p} \in \mathbb{R}^{T \times U}$，然后分别计算question-aware passage和passage-aware question：
	$\mathbf{S}_{p}^{q_{1}}=\mathbf{A Q}$和$\mathbf{Q}_{p}=\mathbf{A}^{\top} \mathbf{S}_{p}$，根据更新的question表示再计算$\mathbf{S}_{p}^{q_{2}} \in \mathbb{R}^{T \times H}$，其中$\mathbf{S}_{p}^{q_{2}}=\mathbf{A} \mathbf{Q}_{p}$

	然后将两次计算的结果拼接：$S_p^q\in \mathbb{R}^{T \times 2 H}=\mathbf{S}_{p}^{q_{1}} \| \mathbf{S}_{p}^{q_{2}}$。
	\begin{equation}
	\begin{array}{l}{a_{t}^{p} \propto \exp \left(\mathbf{s}_{p, t}^{q} \mathbf{w}^{\top}\right)} \\ {\tilde{\mathbf{s}}_{p}=\mathbf{a}^{p} \mathbf{S}_{p}^{q}}\end{array}
	\end{equation}
	\begin{equation}
		\mathbf{x}_{p s g}=\operatorname{FFL}\left(\tilde{\mathbf{s}}_{p 1}, \tilde{\mathbf{s}}_{p_{2}}\right)
		\end{equation}
	\subitem[3] Path Scoring:
	\begin{equation}
		\tilde{\mathbf{q}}=\left(\mathbf{q}_{0} \| \mathbf{q}_{U}\right) \mathbf{W}_{q}
		\end{equation}
		\begin{equation}
			\mathbf{y}_{x_{c t x}, q}=\mathrm{FFL}\left(\mathbf{x}_{c t x}, \tilde{\mathbf{q}}\right)
			\end{equation}
			\begin{equation}
				z_{c t x}=\mathbf{y}_{x_{c t x}, q} \mathbf{w}_{c t x}^{\top}
				\end{equation}
				\begin{equation}
					z_{p s g}=\tilde{\mathbf{c}}_{k} \mathbf{x}_{p s g}^{\top}
					\end{equation}
					\begin{equation}
						z=z_{c t x}+z_{p s g}
						\end{equation}
\end{enumerate}

\newpage
\section{Relational inductive biases, deep learning, and graph networks}
主要介绍了一些GNN的相关应用方法，是Section 1 的论文中System 2的组成部分。
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{3-1.png}
	\caption{GNN的定义}
\end{figure}

GNN中节点更新方式：
\begin{equation}
\begin{array}{ll}{\mathbf{e}_{k}^{\prime}=\phi^{e}\left(\mathbf{e}_{k}, \mathbf{v}_{r_{k}}, \mathbf{v}_{s k}, \mathbf{u}\right)} & {\overline{\mathbf{e}}_{i}^{\prime}=\rho^{e \rightarrow v}\left(E_{i}^{\prime}\right)} \\ {\mathbf{v}_{i}^{\prime}=\phi^{v}\left(\overline{\mathbf{e}}_{i}^{\prime}, \mathbf{v}_{i}, \mathbf{u}\right)} & {\overline{\mathbf{e}}^{\prime}=\rho^{e \rightarrow u}\left(E^{\prime}\right)} \\ {\mathbf{u}^{\prime}=\phi^{u}\left(\overline{\mathbf{e}}^{\prime}, \overline{\mathbf{v}}^{\prime}, \mathbf{u}\right)} & {\overline{\mathbf{v}}^{\prime}=\rho^{v \rightarrow u}\left(V^{\prime}\right)}\end{array}
\end{equation}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{3-2.png}
\end{figure}

\newpage
\section{BAM! Born-Again Multi-Task Networks for Natural Language Understanding}
多任务模型的性能通常比它们的单任务对应对象差,本文就提出了了一种通用的解决方案。通过Knowledge Distillation，Multi-Task Distillation和Teacher Anneall来解决这个问题。
主要思想是对于每一个Task，由Teacher(Teacher的结构和student一样)来“教导”它们该如何去做，在训练一定时间之后，“student”再去向golden answer学习。理由是teacher提供的answer的预测是一个分布，比golden answer的one-hot能够提供更多的信息。
比如在图像分类的时候，该图片为马，one-hot只会标注这是一只马，而Teacher预测的distribution不仅能知道是马，
还能够知道，相比于自行车飞机什么的，更有可能是一只驴。\\
1.Kownledge Distillation:\\

传统的one-hot：$\mathcal{L}(\theta)=\sum_{x_{\tau}^{i}, y_{\tau}^{i} \in \mathcal{D}_{\tau}} \ell\left(y_{\tau}^{i}, f_{\tau}\left(x_{\tau}^{i}, \theta\right)\right)$\\

Distillation:$\mathcal{L}(\theta)=\sum_{x_{\tau}^{i}, y_{\tau}^{i} \in \mathcal{D}_{\tau}} \ell\left(f_{\tau}\left(x_{\tau}^{i}, \theta^{\prime}\right), f_{\tau}\left(x_{\tau}^{i}, \theta\right)\right)$\\
2.Multi-Task Distillation:\\$$
\mathcal{L}(\theta)=\sum_{\tau \in \mathcal{T}} \sum_{x_{\tau}^{i}, y_{\tau}^{i} \in \mathcal{D}_{\tau}} \ell\left(f_{\tau}\left(x_{\tau}^{i}, \theta_{\tau}\right), f_{\tau}\left(x_{\tau}^{i}, \theta\right)\right)
$$
3.Teacher Annealing:
$$
\ell\left(\lambda y_{\tau}^{i}+(1-\lambda) f_{\tau}\left(x_{\tau}^{i}, \theta_{\tau}\right), f_{\tau}\left(x_{\tau}^{i}, \theta\right)\right)
$$
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{4-1.png}
\end{figure}

\newpage
\section{Graph Neural Networks with Generated Parameters for Relation Extraction}
本文是刘知远老师组里的工作，是GNN在推理和关系抽取上的一次应用：Graph Neural Network with Generated Parameter（GP-GNNs）。
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{5-1.png}
\end{figure}
结构分为三部分：
\begin{enumerate}
	\item Encoding Module:\\
	\begin{equation}
		s=\left(x_{0}, x_{1}, \ldots, x_{l-1}\right)
		\end{equation}
		\begin{equation}
			E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]
			\end{equation}
			\begin{equation}
				\mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)
				\end{equation}
	i,j分别是对应的entity的索引，$x_t$是一个sequence中t位置的word。$p_t$是相对位置向量，即表示该单词是否在实体i，实体j中还是都不在。
	\item Propagation Module:\\
	\begin{equation}
		\mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)
		\end{equation}
	\item Classification Module:\\
	\begin{equation}
		\mathcal{L}=g\left(\mathbf{h}_{0 :|\nu|-1}^{0}, \mathbf{h}_{0 :|\nu|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)
		\end{equation}
		\begin{equation}
			\boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]
			\end{equation}
			\begin{equation}
				\mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)
				\end{equation}
\end{enumerate}

\newpage
\section{Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs}
本文所解决的问题是跨文档的多跳阅读理解，提出的模型是Heterogeneous Document-Entity Graph（HDEGraph）。


\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{6-1.png}
\end{figure}
\begin{enumerate}
	\item Context Encoding\\以support document的encoding距离，candidates和query的情况类似。
	\begin{equation}
		\mathbf{A}_{q s}^{i}=\mathbf{H}_{s}^{i}\left(\mathbf{H}_{q}\right)^{\top} \in \mathbb{R}^{l_{s}^{i} \times l_{q}}
		\end{equation}
		\begin{equation}
		\begin{aligned} \mathbf{C}_{q} &=\operatorname{softmax}\left(\mathbf{A}_{q s}^{\top}\right) \mathbf{H}_{s} \in \mathbb{R}^{l_{q} \times h} \\ \mathbf{C}_{s} &=\operatorname{softmax}\left(\mathbf{A}_{q s}\right) \mathbf{H}_{q} \in \mathbb{R}^{l_{s} \times h} \end{aligned}
		\end{equation}
		\begin{equation}
			\mathbf{D}_{s}=f\left(\operatorname{softmax}\left(\mathbf{A}_{q s}\right) C_{q}\right) \in \mathbb{R}^{l_{s} \times h}
			\end{equation}
			\begin{equation}
				\mathbf{S}_{c a}=\left[\mathbf{C}_{s} ; \mathbf{D}_{s}\right] \in \mathbb{R}^{l_{s} \times 2 h}
				\end{equation}
				\begin{equation}
				\begin{array}{c}{\mathbf{a}_{s}=\operatorname{softmax}\left(M L P\left(\mathbf{S}_{c a}\right)\right) \in \mathbb{R}^{l_{s} \times 1}} \\ {\mathbf{s}_{s a}=\mathbf{a}_{s}^{\top} \mathbf{S}_{c a} \in \mathbb{R}^{1 \times 2 h}}\end{array}
				\end{equation}
	\item Reasoning over HDE graph\\
	\begin{equation}
		\mathbf{z}_{i}^{k}=\sum_{r \in \mathcal{R}} \frac{1}{\left|\mathcal{N}_{i}^{r}\right|} \sum_{j \in \mathcal{N}_{i}^{r}} f_{r}\left(\mathbf{h}_{j}^{k}\right)
		\end{equation}
		\begin{equation}
			\mathbf{u}_{i}^{k}=f_{s}\left(\mathbf{h}_{i}^{k}\right)+\mathbf{z}_{i}^{k}
			\end{equation}
			\begin{equation}
			\begin{array}{c}{\mathbf{g}_{i}^{k}=\operatorname{sigmoid}\left(f_{g}\left(\left[\mathbf{u}_{i}^{k} ; \mathbf{h}_{i}^{k}\right]\right)\right)} \\ {\mathbf{h}_{i}^{k+1}=\tanh \left(\mathbf{u}_{i}^{k}\right) \odot \mathbf{g}_{i}^{k}+\mathbf{h}_{i}^{k} \odot\left(1-\mathbf{g}_{i}^{k}\right)}\end{array}
			\end{equation}
	\item Score accumulation\\
	\begin{equation}
		\mathbf{a}=f_{C}\left(\mathbf{H}^{C}\right)+A C C_{\max }\left(f_{E}\left(\mathbf{H}^{E}\right)\right)
		\end{equation}
\end{enumerate}

\newpage
\section{Explore, Propose, and Assemble:
An Interpretable Model for Multi-Hop Reading Comprehension}
本文也是应用于multi-hop multi-document的一篇文章，提出了Explore-Propose-Assemble reader (EPAr)模型。
首先，Document Explorer迭代地选择相关文档并在树结构中表示不同的推理链，以便吸收来自所有链的信息。然后，Answer-Proposer从推理树中的每个根到叶路径提出一个答案。
最后Evidence Assembler从每个路径中提取包含建议答案的关键语句，并将它们组合以预测最终答案。
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{7-1.png}
\end{figure}
\begin{enumerate}
	\item  Retrieval and Encoding:\\使用TF-IDF算法来先筛选出一部分的document，减少计算量。
	\item  Document Explorer：\\用document-level的representation和word-level的representation分别作为Memory Network
	的Key和value。

	Read：\begin{equation}
		x_{n}=p_{n}^{T} \mathbf{W}_{\mathbf{r}} m^{t} \quad \chi=\operatorname{softmax}(x) \quad P\left(d_{i}\right)=\chi_{i}
		\end{equation}
	
	Write：\begin{equation}
	\begin{array}{rlrl}{w_{k}} & {=h_{k}^{T} \mathbf{W}_{\mathbf{w}} m} & {\tilde{h}} & {=\sum_{k=1}^{K} h_{k} \omega_{k}} \\ {\omega} & {=\operatorname{softmax}(w)} & {m^{t+1}} & {=\mathbf{G} \mathbf{R} \mathbf{U}\left(\tilde{h}, m^{t}\right)}\end{array}
	\end{equation}
	\item  Answer Proposer
	\begin{equation}
	\begin{aligned} e_{i}^{k} &=\mathbf{v}^{T} \tanh \left(\mathbf{W}_{\mathbf{h}} \hat{h}_{c c t}^{i}+\mathbf{W}_{\mathbf{s}} s^{k}+\mathbf{b}\right) \\ a^{k} &=\operatorname{softmax}\left(e^{k}\right) ; \quad c^{k}=\sum_{i} a_{i}^{k} h_{c c t}^{i} \\ y^{k} &=\mathbf{L} \mathbf{S} \mathbf{T} \mathbf{M}\left(\hat{h}_{T}^{k-1}, s^{k-1}, c^{k-1}\right) \\ w^{k} &=\boldsymbol{\alpha}\left(y^{k}, u_{s}\right)+\boldsymbol{\alpha}\left(y^{k}, u_{b}\right) ; \epsilon=\operatorname{softmax}(w) \end{aligned}
	\end{equation}
	\begin{equation}
		a=\sum_{k=1}^{K} \hat{h}_{T}^{k} \epsilon_{k} ; \quad \text { Score }_{l}=\boldsymbol{\beta}\left(c_{l}, a\right)
		\end{equation}
	\item  Evidence Assembler：\\将相关的sentence拼接起来，送入传统的单document算法模型。
	\item  Joint Optimization
\end{enumerate}
\newpage
\section{Multi-hop Reading Comprehension
through Question Decomposition and Rescoring}
本文提出了一种创新的方法来解决multi-hop的阅读理解问题，没有在神经网络的结构上做文章，而是在数据上做文章，将原始的mult-hop的question分解文多个single-hop的问题，然后用传统的RC模型去回答问题，然后将答案再进行整合。
代码地址：\url{https://github.com/shmsw25/DecompRC}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{8-1.png}
	\caption{原问题分解的三种目标子类型}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{8-1.png}
	\caption{整体的目标结构}
\end{figure}
\begin{equation}
	U=\operatorname{BERT}(S) \in \mathbb{R}^{n \times h}
	\end{equation}
	\begin{equation}
		Y=\operatorname{softmax}(U W) \in \mathbb{R}^{n \times c}
		\end{equation}

\begin{equation}
\operatorname{ind}_{1}, \ldots, \operatorname{ind}_{c}=\underset{i_{1} \leq \cdots \leq i_{c}}{\operatorname{argmax}} \prod_{j=1}^{c} \mathbb{P}\left(i_{j}=\operatorname{ind}_{j}\right)
\end{equation}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{8-3.png}
	\caption{生成分割点的算法}
\end{figure}

\newpage
\section{Dynamically Fused Graph Network for Multi-hop Reasoning}
本文也是一篇关于Mult-hop的用GNN来处理的文章。亮点在于GNN推理的时候，使用了一个soft-mask，可以理解为只做了一个局部的信息传播。
模型可以划分为以下步骤：
\begin{enumerate}
	\item Paragraph Selection:\\分别将paragraph和Query作为输入送入Bert，然后通过一个SIGMOD来预测其相关性，筛选出部分最相关的paragraph。
	\item Constructing Entity Graph:\\用Stanford coreNLP工具来做实体抽取，作为node，然后以下三种情况nodes间有边：
	\subitem[1] 两个实体出现在同一句话中。
	\subitem[2] 对于在上下文中具有相同提及文本的每对实体。
	\subitem[3] 在同一段中的中央实体节点和其他实体之间。
	\item Encoding Query and Context:使用bert即可。
	\item Reasoning with the Fusion Block：\\
	Document to Graph flow:\\将entity含有的word的embedding通过max和mean池化层得到所需向量。\\
	Dynamic Graph Attention：
	\begin{equation}
	\begin{aligned} \tilde{\mathbf{q}}^{(t-1)} &=\operatorname{MeanP} \operatorname{ooling}\left(\mathbf{Q}^{(t-1)}\right) \\ \gamma_{i}^{(t)} &=\tilde{\mathbf{q}}^{(t-1)} \mathbf{V}^{(t)} \mathbf{e}_{i}^{(t-1)} / \sqrt{d_{2}} \\ \mathbf{m}^{(t)} &=\sigma\left(\left[\gamma_{1}^{(t)}, \cdots, \gamma_{N}^{(t)}\right]\right) \\ \tilde{\mathbf{E}}^{(t-1)} &=\left[m_{1}^{(t)} \mathbf{e}_{1}^{(t-1)}, \ldots, m_{N}^{(t)} \mathbf{e}_{N}^{(t-1)}\right] \end{aligned}
	\end{equation}
	\begin{equation}
	\begin{aligned} \mathbf{h}_{i}^{(t)} &=\mathbf{U}_{t} \tilde{\mathbf{e}}_{i}^{(t-1)}+\mathbf{b}_{t} \\ \beta_{i, j}^{(t)} &=\text { LeakyReLU }\left(\mathbf{W}_{t}^{\top}\left[\mathbf{h}_{i}^{(t)}, \mathbf{h}_{j}^{(t)}\right]\right) \\ \alpha_{i, j}^{(t)} &=\frac{\exp \left(\beta_{i, j}^{(t)}\right)}{\sum_{k} \exp \left(\beta_{i, k}^{(t)}\right)} \end{aligned}
	\end{equation}
	\begin{equation}
		\mathbf{e}_{i}^{(t)}=\operatorname{ReLU}\left(\sum_{j \in B_{i}} \alpha_{j, i}^{(t)} \mathbf{h}_{j}^{(t)}\right)
		\end{equation}
		\begin{equation}
			\mathbf{Q}^{(t)}=\mathrm{Bi}-\mathrm{Attention}\left(\mathbf{Q}^{(t-1)}, \mathbf{E}^{(t)}\right)
			\end{equation}
	\item Graph to Document Flow:\begin{equation}
		\mathbf{C}^{(t)}=\operatorname{LSTM}\left(\left[\mathbf{C}^{(t-1)}, \mathbf{M} \mathbf{E}^{(t) \top}\right]\right)
		\end{equation}
	\item Prediction:
	\begin{equation}
	\begin{aligned} \mathbf{O}_{s u p} &=\mathcal{F}_{0}\left(\mathbf{C}^{(t)}\right) \\ \mathbf{O}_{s t a r t} &=\mathcal{F}_{1}\left(\left[\mathbf{C}^{(t)}, \mathbf{O}_{s u p}\right]\right) \\ \mathbf{O}_{e n d} &=\mathcal{F}_{2}\left(\left[\mathbf{C}^{(t)}, \mathbf{O}_{s u p}, \mathbf{O}_{s t a r t}\right]\right) \\ \mathbf{O}_{t y p e} &=\mathcal{F}_{3}\left(\left[\mathbf{C}^{(t)}, \mathbf{O}_{s u p}, \mathbf{O}_{e n d}\right]\right) \end{aligned}
	\end{equation}
\end{enumerate}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{9-2.png}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{9-3.png}
\end{figure}
\end{document}